## Neural network model {#nn_model}

A neural network model is a general non-parametric
non-linear model that models both non-linear relationships
and interaction effects between input and output data. In
this paper, we built two neural network models to (1) model
the stochastic discount factor (SDF) weight and (2) model
factors unexplained by the no-arbitrage condition. The [GAN
model](#gan_model) section explains the use of these two
neural networks in depth.
Each of the two neural network models combines feed-forward
and recurrent neural networks. We first explain the
standard feed-forward
neural network before expanding to the recurrent neural network
in [long short-term memory (LSTM)
architecture](#LSTM_model). Various training techniques are
employed to avoid model over-fitting and are explained
under [model training](#model_training). 

### Feed forward neural network

A standard feed-forward neural network consists of one input
layer, one or multiple hidden layer(s) and one output layer.
For example, a data $\mathbf{X} = (X_1, X_2, \cdots, X_p)$ with $p$
covariates has $p$ input units in the input layer (first
layer in figure \@ref(fig:feedforward)).
The input
layer is then linked to the hidden units in the hidden
layer, as indicated by the arrow between the first layer and
the second layer.
Finally, the hidden layers are linked to the output layer to
produce predictions, as indicated by the arrows between the
second and last layers.
Our model's input data includes
both macroeconomic factors and firm characteristics data.

![(#fig:feedforward) Feed forward neural network](./img/feedforward)

### Hidden layers and hidden units

Hidden layers refer to the layers between the input
and output layer, and hidden units are the units within
each hidden layer. A deep neural network is a feed-forward
network with multiple hidden layers.
In each hidden layer $k$, there are $j$ hidden units, where
the number of hidden units and the number of hidden layers
are hyper-parameters.
Within the $j$th hidden unit, each
covariate $i$ has an individual weight $\omega_{ij}^{(k)}$,
and all covariates share a common bias $\omega_{0j}^{(k)}$.
This is effectively a linear
regression with $\omega_{0j}^{(k)} + \sum_{i=1}^p
\omega_{ij}^{(k)} X_i$.
An activation function $h(\cdot)$ is then applied to the linear
regression output. Usually, the activation action will be a
non-linear function.
Therefore, the output of each of the hidden units are
$h_j^{(k)}(\omega_{ij}^{(k)}
+ \sum_{i=1}^p \omega_{ij}^{(k)} X_i)$. 

### Activation function

There are various activation functions $h(\cdot)$
available. This paper uses one of the most commonly
used activation functions known as rectified linear unit
(ReLU), shown in Figure \@ref(fig:relu). $ReLU(x) := \max(x,
0)$ effectively removes the negative values. It is shown to
improve the convergence rate of stochastic gradient descent
compared to other activation functions such as the sigmoid
function [@nair_rectified_2010].

```{r relu, fig.cap="Rectified linear unit (ReLU) activation function", echo=F}
data <- -5:5
data <- ifelse(data <= 0, 0, data)
plot(-5:5, data, type = "l", xlab = "x", ylab = "ReLU(x)")
```

### Loss function

After the data is passed from the input layer to the output
layer, a loss function calculates the loss in the model. The
loss summarises the training objective.
One commonly used loss function is
mean squared error $E(\mathbf{y} - \mathbf{\hat{y}})$,
which minimises the mean squared distance between actual
output and predictions.
This paper implemented a custom pricing loss
function described in the [pricing loss
function](#loss_function) section.

### Back-propagation

After the loss is calculated, a back-propagation training
algorithm is then used to train the network.
Back-propagation algorithm minimises a given loss function
by updating the weights and biases in the network model
through gradient descent methods. This paper adopted
adaptive moment estimation (Adam) as the optimiser as
described in [training neural network](#adam) section.
