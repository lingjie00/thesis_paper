## Long short-term memory network architecture {#LSTM_model}

A recurrent neural network (RNN) is a class of neural
networks that considers the past data in future prediction
and hence is particularly suited for the prediction of time
series data.
Long Short-Term Memory (LSTM) is a sub-class of RNN that
considers more extended dynamics through a gate controlling
system.
We focus our discussion here on LSTM.
In contrast to a feed-forward network that receives only
input data $\mathbf{x}_{(t)}$ at time $t$,
an LSTM also gets a short term state
$\mathbf{h}_{(t)}$ and a long term state $\mathbf{c}_{(t)}$.

As shown in figure \@ref(fig:lstmcell), the long term state
$\mathbf{c}_{(t-1)}$ from the previous iteration travel
through a forget gate and an input
gate before being passed through to the next iteration.
The long term state $\mathbf{c}_{(t)}$ is also being passed
as a short term state $\mathbf{h}_{(t)}$ through an output
gate (which is also the output of the cell $\mathbf{y}_{(t)}$).
The gates are being controlled by the functions
$\mathbf{f}_{(t)}, \mathbf{i}_{(t)}, \mathbf{o}_{(t)}$, with
logistic activation functions that produce $1$ to keep the
gate open and $0$ to close the gate. The decision on the
gates is based on the input data $\mathbf{x}_{(t)}$ and
short term state $\mathbf{h}_{(t)}$. The function
$\mathbf{q}_{(t)}$ takes in the data and short term state to
be combined with the long term state.
Summarising the LSTM computation, we have the following
equations. $\mathbf{W}_{x\cdot}$ are the weights for
$\mathbf{x}_{(t)}$, and $\mathbf{W}_{h\cdot}$ are the weights
for $\mathbf{h}_{(t-1)}$ and $\mathbf{b}_i$ are the bias for
$i$ layers, $\otimes$ are element-wise multiplication.
Therefore, using an LSTM neural network layer allows the GAN
model to extract long term dynamics from macroeconomics data
without specifying the number of lags as a hyper-parameter.

![(#fig:lstmcell) Long short-term memory cell](./src/lstm)

\begin{align*}
\mathbf{i}_{(t)} &= \sigma(
    \mathbf{W}_{xi}^T \mathbf{x}_{(t)}
    + \mathbf{W}_{hi}^T\mathbf{h}_{(t-1)} + \mathbf{b}_i
) \\
\mathbf{f}_{(t)} &= \sigma(
    \mathbf{W}_{xf}^T\mathbf{x}_{(t)} + \mathbf{W}_{hf}^T
    \mathbf{h}_{(t-1)} + \mathbf{b}_f
) \\
\mathbf{o}_{(t)} &= \sigma(
    \mathbf{W}_{xo}^T \mathbf{x}_{(t)} +
    \mathbf{W}_{ho}^T\mathbf{h}_{(t-1)} + \mathbf{b}_o
) \\
\mathbf{q}_{(t)} &= tanh(
    \mathbf{W}_{xg}^T \mathbf{x}_{(t)} +
    \mathbf{W}^T_{hg}\mathbf{h}_{(t-1)} + \mathbf{b}_g
) \\
\mathbf{c}_{(t)} &= \mathbf{f}_{(t)} \otimes \mathbf{c}_{(t-1)} + 
    \mathbf{i}_{(t)}\otimes \mathbf{q}_{(t)}\\
\mathbf{y}_{(t)} &= \mathbf{h}_{(t)} = \mathbf{o}_{(t)}
\otimes tanh(\mathbf{c}_{(t)})
\end{align*}

Algorithms adapted from @geron_hands-machine_2017


